---
layout: default
---

# Workshop Description

Welcome to the world of Big Data!

Every day, more than 370 millions of TB of data is generated in the world. Extracting value out of it requires us to understand all the different ways it can be stored and processed. This workshop is designed to help everyone, from curious learners to seasoned professionals, understand Big Data and the profound implications it has across industries.

Together, we will discover the complexities of Big Data, dive into its ecosystem, and get hands-on experience with tools like Apache Spark and its Python library. Our objective is to give you a solid overview of the Big Data landscape and delve into the use of one of its most important tools - Spark. We will also teach you how the latest AI tools can be used to improve your efficiency in processing data.

This workshop isn't just about learning—it's about truly experiencing the world of Big Data in a modern way. Join us for an enlightening journey!

This workshop will assume **very basic knowledge of programming in Python** for the few technical use-cases we will cover. Ability to use notebooks / Google Colab would help you get to work faster, but we will cover that briefly if you have never used it. No need to install anything else!

# Workshop Materials

## Pre-workshop Checklist

To join the workshop, you'll need a device with a recent web browser and two-way audio and video access to Zoom.

No further tool is required to be installed beforehand.

If you don't have experience with Python, we recommend you to take the Introduction to Python workshop before this one!

## Schedule

#### July 26 (1:00 - 4 P.M. PST) : Introduction and Data Storage Concepts

1. Introduction to Big Data: Definition, Relevance, Challenges
2. Real-world Applications of Big Data
3. Overview of Different Types of Data Storage
4. Hands-on Activities with Data Storage: SQL, NoSQL, Warehouses, and Data Lakes
5. Q&A and Wrap Up

#### July 27 (1:00 - 4 P.M. PST) : Big Data Processing and Future Directions

1. Apache Spark: Definition, Relevance, Use Cases and other tools
2. Hands-on Activities: Basic Data Loading and Processing with PySpark
3. Relating PySpark to more traditional data science tools
4. The Future of Big Data & how to use AI to improve your efficiency
5. Closing Remarks, Q&A & recommendations for further learning

## Additional Resources

We will add here additional resources that we think are useful for you to learn more about Big Data.

## About the Instructors

![Axel](/assets/img/profile_axel.png){:style="max-width:30%;"}

[LinkedIn](https://www.linkedin.com/in/axel-peytavin/)

Axel Peytavin earned his M.S. from ICME in June 2023, and is currently pursuing research on Twitter data with Professor Johan Ugander and Professor Martin Saveski in Stanford’s department of Management Science & Engineering. He is also a volunteer for The Ocean Cleanup, for which we worked as a computational modeler prior to coming to Stanford, and a co-founder of the GetAlong project to bring better discussions on information on the internet.

![Julia](/assets/img/profile_julia.png){:style="max-width:30%;"}

[LinkedIn](https://www.linkedin.com/in/anna-julia-storch-517142144/)

Anna-Julia Storch recently earned her M.S. in Education Data Science from Stanford, exploring how data science can help us solve educational challenges. She has also been a teaching assistant for Stanford's premier entrepreneurship courses including "The Lean Launchpad" (taught be Steve Blank). Prior to Stanford, she has worked in a variety of roles including as a digital consultant for a big data project at McKinsey, in VC-backed technology startups and as a business line head for a multinational HR company.
